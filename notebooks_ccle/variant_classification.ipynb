{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6181e92",
   "metadata": {},
   "source": [
    "# Variant Classification: Model Comparison\n",
    "\n",
    "This notebook compares several classifiers for predicting variant types using different preprocessed datasets. All random processes use a fixed random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0267a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856667d",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "Define a function to load and preprocess each dataset. This ensures consistent handling of columns, missing values, splitting, and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(path, index_col=None, drop_cols=None):\n",
    "    \"\"\"\n",
    "    Loads a CSV file and applies standard preprocessing:\n",
    "    - Drops specified columns if present.\n",
    "    - Drops 'Unnamed: 0' if present.\n",
    "    - Drops 'Mutated' if present (not a target here).\n",
    "    - Drops non-numeric columns (e.g., ModelID).\n",
    "    - Splits into train/test and scales features.\n",
    "    Returns: (X_train_scaled, X_test_scaled, y_train, y_test, feature_names)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, index_col=index_col)\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols, errors='ignore')\n",
    "    df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "    df = df.drop(columns=['Mutated'], errors='ignore')\n",
    "    # Separate target before selecting numeric features\n",
    "    target = df['VariantType']\n",
    "    features = df.drop(columns=['VariantType'], errors='ignore')\n",
    "    # Keep only numeric features\n",
    "    features = features.select_dtypes(include=[np.number])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=0.2, random_state=RANDOM_SEED\n",
    "    )\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4c1fe",
   "metadata": {},
   "source": [
    "## Prepare Datasets\n",
    "\n",
    "Load all datasets and store them in a list for easy iteration. Each entry contains scaled train/test splits and feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5df4db2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['VariantType'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m datasets = []\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path, idx_col \u001b[38;5;129;01min\u001b[39;00m dataset_paths:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     X_train, X_test, y_train, y_test, feature_names = \u001b[43mload_and_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43midx_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdrop_cols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     datasets.append((X_train, X_test, y_train, y_test, feature_names))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mload_and_preprocess\u001b[39m\u001b[34m(path, index_col, drop_cols)\u001b[39m\n\u001b[32m     14\u001b[39m df = df.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mMutated\u001b[39m\u001b[33m'\u001b[39m], errors=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Drop non-numeric columns (e.g., ModelID)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m df = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mVariantType\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m features = df.drop(columns=[\u001b[33m'\u001b[39m\u001b[33mVariantType\u001b[39m\u001b[33m'\u001b[39m], errors=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     18\u001b[39m target = df[\u001b[33m'\u001b[39m\u001b[33mVariantType\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/pandas/core/frame.py:10757\u001b[39m, in \u001b[36mDataFrame.join\u001b[39m\u001b[34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[39m\n\u001b[32m  10747\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m how == \u001b[33m\"\u001b[39m\u001b[33mcross\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m  10748\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[32m  10749\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10750\u001b[39m             other,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10755\u001b[39m             validate=validate,\n\u001b[32m  10756\u001b[39m         )\n\u001b[32m> \u001b[39m\u001b[32m10757\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10758\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10759\u001b[39m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10760\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10761\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10762\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m  10763\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m  10764\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlsuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrsuffix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10765\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10766\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10767\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  10768\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m  10769\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/pandas/core/reshape/merge.py:184\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/pandas/core/reshape/merge.py:888\u001b[39m, in \u001b[36m_MergeOperation.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right = \u001b[38;5;28mself\u001b[39m._indicator_pre_merge(\u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right)\n\u001b[32m    886\u001b[39m join_index, left_indexer, right_indexer = \u001b[38;5;28mself\u001b[39m._get_join_info()\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[38;5;28mself\u001b[39m._merge_type)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.indicator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/pandas/core/reshape/merge.py:840\u001b[39m, in \u001b[36m_MergeOperation._reindex_and_concat\u001b[39m\u001b[34m(self, join_index, left_indexer, right_indexer, copy)\u001b[39m\n\u001b[32m    837\u001b[39m left = \u001b[38;5;28mself\u001b[39m.left[:]\n\u001b[32m    838\u001b[39m right = \u001b[38;5;28mself\u001b[39m.right[:]\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m llabels, rlabels = \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mright\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msuffixes\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[32m    845\u001b[39m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[32m    846\u001b[39m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[32m    847\u001b[39m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[32m    848\u001b[39m     lmgr = left._mgr.reindex_indexer(\n\u001b[32m    849\u001b[39m         join_index,\n\u001b[32m    850\u001b[39m         left_indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    855\u001b[39m         use_na_proxy=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    856\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/pandas/core/reshape/merge.py:2721\u001b[39m, in \u001b[36m_items_overlap_with_suffix\u001b[39m\u001b[34m(left, right, suffixes)\u001b[39m\n\u001b[32m   2718\u001b[39m lsuffix, rsuffix = suffixes\n\u001b[32m   2720\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lsuffix \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rsuffix:\n\u001b[32m-> \u001b[39m\u001b[32m2721\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcolumns overlap but no suffix specified: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto_rename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2723\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mrenamer\u001b[39m(x, suffix: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2724\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2725\u001b[39m \u001b[33;03m    Rename the left and right indices.\u001b[39;00m\n\u001b[32m   2726\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2737\u001b[39m \u001b[33;03m    x : renamed column name\u001b[39;00m\n\u001b[32m   2738\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: columns overlap but no suffix specified: Index(['VariantType'], dtype='object')"
     ]
    }
   ],
   "source": [
    "dataset_paths = [\n",
    "    ('../data/processed/ccle_quantile_filtered.csv', None),\n",
    "    ('../data/processed/ccle_tp53_filtered.csv', None),\n",
    "    ('../data/processed/ccle_variance_filtered.csv', None),\n",
    "    ('../data/processed/merged_data.csv', 1)\n",
    "]\n",
    "drop_cols = ['VariantLabel']\n",
    "\n",
    "datasets = []\n",
    "for path, idx_col in dataset_paths:\n",
    "    X_train, X_test, y_train, y_test, feature_names = load_and_preprocess(path, index_col=idx_col, drop_cols=drop_cols)\n",
    "    datasets.append((X_train, X_test, y_train, y_test, feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c19847",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Function\n",
    "\n",
    "This function trains and evaluates a given model on each dataset, displaying accuracy, F1 score, confusion matrix, and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d5a392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_on_datasets(datasets, model_name):\n",
    "    base_model_map = {\n",
    "        'logistic': LogisticRegression(max_iter=1000, solver='lbfgs', random_state=RANDOM_SEED),\n",
    "        'svm': SVC(kernel='rbf', probability=True, decision_function_shape='ovr', random_state=RANDOM_SEED),\n",
    "        'random_forest': RandomForestClassifier(random_state=RANDOM_SEED),\n",
    "        'catboost': CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, random_seed=RANDOM_SEED, verbose=0),\n",
    "        'xgboost': XGBClassifier(eval_metric='mlogloss', random_state=RANDOM_SEED),\n",
    "    }\n",
    "\n",
    "    if model_name not in base_model_map:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    # Wrap logistic in One-vs-Rest\n",
    "    if model_name == 'logistic':\n",
    "        model = OneVsRestClassifier(base_model_map['logistic'])\n",
    "    else:\n",
    "        model = base_model_map[model_name]\n",
    "\n",
    "    num_datasets = len(datasets)\n",
    "    fig, axes = plt.subplots(nrows=num_datasets, ncols=2, figsize=(12, 5 * num_datasets))\n",
    "    if num_datasets == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, (X_train, X_test, y_train, y_test, feature_names) in enumerate(datasets):\n",
    "        # Train the model\n",
    "        if model_name in ['catboost', 'xgboost']:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = classification_report(y_test, y_pred, output_dict=True, zero_division=0)['weighted avg']['f1-score']\n",
    "\n",
    "        print(f\"Dataset {idx+1} - Accuracy: {acc:.2f} - F1 Score: {f1:.2f} - Model: {model_name}\")\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx][0])\n",
    "        axes[idx][0].set_title(f'Dataset {idx+1} - Confusion Matrix')\n",
    "        axes[idx][0].set_xlabel('Predicted')\n",
    "        axes[idx][0].set_ylabel('True')\n",
    "\n",
    "        # Feature importance\n",
    "        if model_name in ['random_forest', 'catboost', 'xgboost']:\n",
    "            importances = model.feature_importances_\n",
    "        elif model_name == 'svm':\n",
    "            if hasattr(model, 'coef_'):\n",
    "                importances = np.abs(model.coef_).sum(axis=0)\n",
    "            else:\n",
    "                axes[idx][1].axis('off')\n",
    "                continue\n",
    "        elif model_name == 'logistic':\n",
    "            if hasattr(model, 'estimators_'):\n",
    "                all_coefs = np.array([np.abs(est.coef_).flatten() for est in model.estimators_])\n",
    "                importances = all_coefs.sum(axis=0)\n",
    "            else:\n",
    "                axes[idx][1].axis('off')\n",
    "                continue\n",
    "        else:\n",
    "            axes[idx][1].axis('off')\n",
    "            continue\n",
    "\n",
    "        indices = np.argsort(importances)[::-1][:10]\n",
    "        top_features = feature_names[indices]\n",
    "        sns.barplot(x=importances[indices], y=top_features, ax=axes[idx][1])\n",
    "        axes[idx][1].set_title(f'Dataset {idx+1} - Feature Importance')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e849c9",
   "metadata": {},
   "source": [
    "## Logistic Regression (One-vs-Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_on_datasets(datasets, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a317f49",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_on_datasets(datasets, 'svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2089bd90",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_on_datasets(datasets, 'random_forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bedd45",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_on_datasets(datasets, 'xgboost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
